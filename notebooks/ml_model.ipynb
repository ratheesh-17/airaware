{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb359d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook cwd: c:\\Users\\91934\\airaware\\notebooks\n",
      "Expected station_hour path: ..\\datasets\\station_hour.csv\n",
      "Exists? True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "ROOT = Path(\"..\")       # notebook folder -> project root\n",
    "DATA_DIR = ROOT / \"datasets\"\n",
    "STATION_HOUR_CSV = DATA_DIR / \"station_hour.csv\"\n",
    "\n",
    "print(\"Notebook cwd:\", Path.cwd())\n",
    "print(\"Expected station_hour path:\", STATION_HOUR_CSV)\n",
    "print(\"Exists?\", STATION_HOUR_CSV.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ae9e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading first 5 rows...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StationId</th>\n",
       "      <th>Datetime</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQI_Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-24 17:00:00</td>\n",
       "      <td>60.50</td>\n",
       "      <td>98.00</td>\n",
       "      <td>2.35</td>\n",
       "      <td>30.80</td>\n",
       "      <td>18.25</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>11.85</td>\n",
       "      <td>126.40</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-24 18:00:00</td>\n",
       "      <td>65.50</td>\n",
       "      <td>111.25</td>\n",
       "      <td>2.70</td>\n",
       "      <td>24.20</td>\n",
       "      <td>15.07</td>\n",
       "      <td>9.77</td>\n",
       "      <td>0.1</td>\n",
       "      <td>13.17</td>\n",
       "      <td>117.12</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-24 19:00:00</td>\n",
       "      <td>80.00</td>\n",
       "      <td>132.00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>25.18</td>\n",
       "      <td>15.15</td>\n",
       "      <td>12.02</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12.08</td>\n",
       "      <td>98.98</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.98</td>\n",
       "      <td>0.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-24 20:00:00</td>\n",
       "      <td>81.50</td>\n",
       "      <td>133.25</td>\n",
       "      <td>1.95</td>\n",
       "      <td>16.25</td>\n",
       "      <td>10.23</td>\n",
       "      <td>11.58</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.47</td>\n",
       "      <td>112.20</td>\n",
       "      <td>0.2</td>\n",
       "      <td>6.72</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AP001</td>\n",
       "      <td>2017-11-24 21:00:00</td>\n",
       "      <td>75.25</td>\n",
       "      <td>116.00</td>\n",
       "      <td>1.43</td>\n",
       "      <td>17.48</td>\n",
       "      <td>10.43</td>\n",
       "      <td>12.03</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9.12</td>\n",
       "      <td>106.35</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StationId             Datetime  PM2.5    PM10    NO    NO2    NOx    NH3  \\\n",
       "0     AP001  2017-11-24 17:00:00  60.50   98.00  2.35  30.80  18.25   8.50   \n",
       "1     AP001  2017-11-24 18:00:00  65.50  111.25  2.70  24.20  15.07   9.77   \n",
       "2     AP001  2017-11-24 19:00:00  80.00  132.00  2.10  25.18  15.15  12.02   \n",
       "3     AP001  2017-11-24 20:00:00  81.50  133.25  1.95  16.25  10.23  11.58   \n",
       "4     AP001  2017-11-24 21:00:00  75.25  116.00  1.43  17.48  10.43  12.03   \n",
       "\n",
       "    CO    SO2      O3  Benzene  Toluene  Xylene  AQI  AQI_Bucket  \n",
       "0  0.1  11.85  126.40      0.1     6.10    0.10  NaN         NaN  \n",
       "1  0.1  13.17  117.12      0.1     6.25    0.15  NaN         NaN  \n",
       "2  0.1  12.08   98.98      0.2     5.98    0.18  NaN         NaN  \n",
       "3  0.1  10.47  112.20      0.2     6.72    0.10  NaN         NaN  \n",
       "4  0.1   9.12  106.35      0.2     5.75    0.08  NaN         NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns:\n",
      "['StationId', 'Datetime', 'PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"..\") / \"datasets\"\n",
    "STATION_HOUR_CSV = DATA_DIR / \"station_hour.csv\"\n",
    "\n",
    "# quick head + columns (very fast)\n",
    "pd.options.display.max_columns = 200\n",
    "print(\"Reading first 5 rows...\")\n",
    "df_head = pd.read_csv(STATION_HOUR_CSV, nrows=5)\n",
    "display(df_head)\n",
    "print(\"\\nColumns:\")\n",
    "print(df_head.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d333eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station_hour path: ..\\datasets\\station_hour.csv\n",
      "SAMPLE_CITY: Delhi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91934\\AppData\\Local\\Temp\\ipykernel_24284\\3292538156.py:21: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(STATION_HOUR_CSV, chunksize=CHUNKSIZE):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No rows found for city filter. Loading first chunk as sample.\n",
      "Loaded shape: (200000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 â€” parameters and load (sample-friendly)\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\"..\")\n",
    "DATA_DIR = ROOT / \"datasets\"\n",
    "STATION_HOUR_CSV = DATA_DIR / \"station_hour.csv\"\n",
    "\n",
    "# --- Config: set SAMPLE_CITY to a city name for quick iteration, or None to load all (may be heavy) ---\n",
    "SAMPLE_CITY = \"Delhi\"   # set to None to load full dataset (may need lots of RAM)\n",
    "CHUNKSIZE = 200000      # for chunk reading when filtering by city\n",
    "\n",
    "print(\"station_hour path:\", STATION_HOUR_CSV)\n",
    "print(\"SAMPLE_CITY:\", SAMPLE_CITY)\n",
    "\n",
    "if SAMPLE_CITY:\n",
    "    # load in chunks and filter by city (case-insensitive)\n",
    "    chunks = []\n",
    "    for chunk in pd.read_csv(STATION_HOUR_CSV, chunksize=CHUNKSIZE):\n",
    "        if \"City\" in chunk.columns:\n",
    "            mask = chunk[\"City\"].str.contains(SAMPLE_CITY, case=False, na=False)\n",
    "            filtered = chunk[mask]\n",
    "            if not filtered.empty:\n",
    "                chunks.append(filtered)\n",
    "    if len(chunks) == 0:\n",
    "        print(\"No rows found for city filter. Loading first chunk as sample.\")\n",
    "        df = pd.read_csv(STATION_HOUR_CSV, nrows=CHUNKSIZE)\n",
    "    else:\n",
    "        df = pd.concat(chunks, ignore_index=True)\n",
    "else:\n",
    "    # load full file (be careful on low-RAM machines)\n",
    "    df = pd.read_csv(STATION_HOUR_CSV)\n",
    "\n",
    "print(\"Loaded shape:\", df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28aacdd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after sanitization:\n",
      " ['StationId', 'Datetime', 'PM2_5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', 'AQI_Bucket']\n",
      "Datetime sample: 0   2017-11-24 17:00:00\n",
      "1   2017-11-24 18:00:00\n",
      "2   2017-11-24 19:00:00\n",
      "3   2017-11-24 20:00:00\n",
      "4   2017-11-24 21:00:00\n",
      "Name: Datetime, dtype: datetime64[ns]\n",
      "After sort shape: (200000, 16)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 â€” sanitize and datetime\n",
    "# Replace spaces/dots in column names to safe identifiers, keep original column mapping in mind\n",
    "df.columns = [c.strip().replace(\" \", \"_\").replace(\".\", \"_\") for c in df.columns]\n",
    "print(\"Columns after sanitization:\\n\", df.columns.tolist())\n",
    "\n",
    "# Ensure Datetime column exists (we saw 'Datetime')\n",
    "if \"Datetime\" not in df.columns:\n",
    "    raise RuntimeError(\"Datetime column not found after sanitize. Check column names.\")\n",
    "\n",
    "df[\"Datetime\"] = pd.to_datetime(df[\"Datetime\"], errors=\"coerce\")\n",
    "print(\"Datetime sample:\", df[\"Datetime\"].head())\n",
    "\n",
    "# Sort per station for time-series ops\n",
    "if \"StationId\" in df.columns:\n",
    "    df = df.sort_values([\"StationId\", \"Datetime\"]).reset_index(drop=True)\n",
    "else:\n",
    "    df = df.sort_values([\"Datetime\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"After sort shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ed498c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag columns sample: ['PM2_5_lag1', 'PM2_5_lag2', 'PM2_5_lag3', 'PM2_5_roll3']\n",
      "NaNs from lags: {'PM2_5_lag1': 40537, 'PM2_5_lag2': 40547, 'PM2_5_lag3': 40557, 'PM2_5_roll3': 36927}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91934\\AppData\\Local\\Temp\\ipykernel_24284\\1667072386.py:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(\"StationId\").apply(make_lags).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 â€” time features and lags\n",
    "df[\"hour\"] = df[\"Datetime\"].dt.hour\n",
    "df[\"day\"] = df[\"Datetime\"].dt.day\n",
    "df[\"month\"] = df[\"Datetime\"].dt.month\n",
    "df[\"dayofweek\"] = df[\"Datetime\"].dt.dayofweek\n",
    "\n",
    "LAGS = [1, 2, 3]\n",
    "ROLL_WINDOW = 3\n",
    "\n",
    "# Create lags for main target PM2_5 (and create for AQI optionally)\n",
    "target_col = \"PM2_5\"    # primary numeric target\n",
    "extra_lag_cols = [\"AQI\"] if \"AQI\" in df.columns else []\n",
    "\n",
    "lag_source_cols = [target_col] + extra_lag_cols\n",
    "\n",
    "# Add lags and rolling mean per station\n",
    "def make_lags(group):\n",
    "    group = group.sort_values(\"Datetime\")\n",
    "    for col in lag_source_cols:\n",
    "        for lag in LAGS:\n",
    "            group[f\"{col}_lag{lag}\"] = group[col].shift(lag)\n",
    "        group[f\"{col}_roll{ROLL_WINDOW}\"] = group[col].rolling(window=ROLL_WINDOW, min_periods=1).mean()\n",
    "    return group\n",
    "\n",
    "if \"StationId\" in df.columns:\n",
    "    df = df.groupby(\"StationId\").apply(make_lags).reset_index(drop=True)\n",
    "else:\n",
    "    df = make_lags(df)\n",
    "\n",
    "# Show how many NaNs introduced by lags\n",
    "lag_cols = [f\"{target_col}_lag{l}\" for l in LAGS] + [f\"{target_col}_roll{ROLL_WINDOW}\"]\n",
    "print(\"Lag columns sample:\", lag_cols)\n",
    "print(\"NaNs from lags:\", df[lag_cols].isnull().sum().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab18487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After creating targets: model dataset shape: (147964, 31)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6 â€” create +1/+2/+3 hour targets and drop rows missing them\n",
    "HORIZONS = [1, 2, 3]\n",
    "for h in HORIZONS:\n",
    "    df[f\"PM2_5_t_plus_{h}\"] = df.groupby(\"StationId\")[target_col].shift(-h)\n",
    "\n",
    "# Drop rows that have NaN in any target or in required lag features\n",
    "required_cols_for_model = lag_cols + [target_col]  # these must be non-null\n",
    "target_cols = [f\"PM2_5_t_plus_{h}\" for h in HORIZONS]\n",
    "df_model = df.dropna(subset=required_cols_for_model + target_cols).reset_index(drop=True)\n",
    "\n",
    "print(\"After creating targets: model dataset shape:\", df_model.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "022fed58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature columns (count): 19\n",
      "['PM2_5', 'PM2_5_lag1', 'PM2_5_lag2', 'PM2_5_lag3', 'PM2_5_roll3', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'hour', 'dayofweek', 'month']\n",
      "X shape: (147964, 19)\n",
      "y shape: (147964, 3)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 â€” select features & targets\n",
    "# Build a conservative feature list â€” include lags, rolling mean, some pollutant cols, and time features\n",
    "feature_cols = []\n",
    "\n",
    "# Add PM2_5 and its lags/roll\n",
    "feature_cols += [target_col] + [f\"{target_col}_lag{l}\" for l in LAGS] + [f\"{target_col}_roll{ROLL_WINDOW}\"]\n",
    "\n",
    "# Add other pollutant columns if present\n",
    "for c in [\"PM10\",\"NO\",\"NO2\",\"NOx\",\"NH3\",\"CO\",\"SO2\",\"O3\",\"Benzene\",\"Toluene\",\"Xylene\"]:\n",
    "    if c in df_model.columns:\n",
    "        feature_cols.append(c)\n",
    "\n",
    "# Add time features\n",
    "feature_cols += [\"hour\", \"dayofweek\", \"month\"]\n",
    "\n",
    "# Ensure uniqueness & presence\n",
    "feature_cols = [c for c in feature_cols if c in df_model.columns]\n",
    "print(\"Final feature columns (count):\", len(feature_cols))\n",
    "print(feature_cols[:40])\n",
    "\n",
    "X = df_model[feature_cols].astype(float)\n",
    "y = df_model[[f\"PM2_5_t_plus_{h}\" for h in HORIZONS]].astype(float)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a1cc6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: (125769, 19) (125769, 3)\n",
      "Test shapes: (22195, 19) (22195, 3)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 â€” time-based train/test split\n",
    "# Use a simple time split to avoid leakage: keep last 15% as test\n",
    "split_idx = int(len(df_model) * 0.85)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shapes:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d1a0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tuned XGBoost modelâ€¦ (will take some time)\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 â€” train model\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Tuned parameters for AQI multi-step prediction\n",
    "xgb_params = {\n",
    "    \"n_estimators\": 600,        # Higher for better accuracy\n",
    "    \"learning_rate\": 0.05,      # Smaller for smoother learning\n",
    "    \"max_depth\": 8,             # Can capture complex patterns\n",
    "    \"subsample\": 0.8,           # Avoid overfitting\n",
    "    \"colsample_bytree\": 0.8,    # Feature sampling\n",
    "    \"reg_alpha\": 0.1,           # L1 regularization (helps with sparsity)\n",
    "    \"reg_lambda\": 1.0,          # L2 regularization\n",
    "    \"min_child_weight\": 3,      # Prevents overfitting small data splits\n",
    "    \"gamma\": 0.1,               # Minimum loss reduction for further partition\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "    \"tree_method\": \"hist\"       # Faster on large datasets\n",
    "}\n",
    "\n",
    "xgbr = XGBRegressor(**xgb_params)\n",
    "\n",
    "# Multi-output wrapper for predicting multiple future hours\n",
    "model = MultiOutputRegressor(xgbr)\n",
    "\n",
    "print(\"Training tuned XGBoost modelâ€¦ (will take some time)\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Training finished.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a066044b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for +1 hour: 25.959\n",
      "MAE for +2 hour: 38.100\n",
      "MAE for +3 hour: 44.680\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 â€” evaluation\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "pred = model.predict(X_test)\n",
    "mae_vals = [mean_absolute_error(y_test.iloc[:,i], pred[:,i]) for i in range(pred.shape[1])]\n",
    "\n",
    "for i, mae in enumerate(mae_vals, start=1):\n",
    "    print(f\"MAE for +{i} hour: {mae:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89f0812a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to: ..\\backend\\app\\ml_model\\aqi_rfr_multi.joblib\n",
      "Saved feature list to: ..\\backend\\app\\ml_model\\feature_columns.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 11 â€” save model and feature list\n",
    "import joblib\n",
    "\n",
    "MODEL_DIR = Path(\"..\") / \"backend\" / \"app\" / \"ml_model\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"aqi_rfr_multi.joblib\"\n",
    "FEATURES_PATH = MODEL_DIR / \"feature_columns.txt\"\n",
    "\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "with open(FEATURES_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for col in feature_cols:\n",
    "        f.write(col + \"\\n\")\n",
    "\n",
    "print(\"Saved model to:\", MODEL_PATH)\n",
    "print(\"Saved feature list to:\", FEATURES_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
